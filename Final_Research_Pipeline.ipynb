{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaaYassinDev/Behavioural-Anomaly-Detection-for-ATO-Fraud/blob/main/Final_Research_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C7B8pFi04Vr",
        "outputId": "9a158e52-8bf4-4e6d-bccf-b3ac4445e007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Model Imports\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from xgboost import XGBClassifier # Import the new model\n",
        "# Ignore unimportant warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Starting Final Research Pipeline ---\")\n",
        "print(\"Objective: Run all experiments step-by-step on the full, original dataset.\")\n",
        "\n",
        "# --- Step 1: Load Full Original Data ---\n",
        "# This is the file you downloaded from Kaggle (6.3 million rows)\n",
        "file_path = '/content/drive/My Drive/Colab_Data/PS_20174392719_1491204439457_log.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"\\n--- Step 1: Load Data ---\")\n",
        "    print(f\"Successfully loaded the full file ({len(df)} rows).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Could not load file '{file_path}'.\")\n",
        "    print(\"Please make sure you have uploaded the correct file from Kaggle to Colab.\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 2: The \"Critical Pivot\" (The Proof) ---\n",
        "print(\"\\n--- Step 2: The 'Critical Pivot' (Proof) ---\")\n",
        "# We prove that profiling 'nameOrig' is impossible\n",
        "sender_counts = df['nameOrig'].value_counts()\n",
        "one_time_senders = sender_counts[sender_counts == 1].count()\n",
        "total_senders = sender_counts.count()\n",
        "one_time_sender_ratio = one_time_senders / total_senders\n",
        "print(f\"Total Senders: {total_senders}\")\n",
        "print(f\"Senders with only 1 transaction: {one_time_senders}\")\n",
        "print(f\"Result: {one_time_sender_ratio:.2%} of senders have no history.\")\n",
        "print(\"Conclusion: Methodology MUST pivot to profiling the recipient ('nameDest').\")\n",
        "\n",
        "# --- Step 3: Build Behavioral Profiles (The \"Smarter\" Way) ---\n",
        "print(\"\\n--- Step 3: Building Behavioral Profiles (from all 6.3M rows) ---\")\n",
        "\n",
        "# (a) Calculate basic statistics (Ratio, Senders)\n",
        "print(\"   (a) Calculating cash-out ratio and unique senders...\")\n",
        "df_received = df[df['type'].isin(['TRANSFER', 'CASH_IN'])]\n",
        "total_received = df_received.groupby('nameDest')['amount'].sum().to_dict()\n",
        "unique_senders = df_received.groupby('nameDest')['nameOrig'].nunique().to_dict()\n",
        "df_cashed_out = df[df['type'] == 'CASH_OUT']\n",
        "total_cashed_out = df_cashed_out.groupby('nameOrig')['amount'].sum().to_dict()\n",
        "\n",
        "all_user_ids = set(total_received.keys()) | set(total_cashed_out.keys()) | set(unique_senders.keys())\n",
        "profiles_list = []\n",
        "for user_id in all_user_ids:\n",
        "    received = total_received.get(user_id, 0)\n",
        "    cashed_out = total_cashed_out.get(user_id, 0)\n",
        "    senders = unique_senders.get(user_id, 0)\n",
        "    ratio = (cashed_out / (received + 1e-6))\n",
        "    ratio = min(ratio, 1.0)\n",
        "    profiles_list.append({\n",
        "        'user_id': user_id,\n",
        "        'dest_cash_out_ratio': ratio,\n",
        "        'dest_unique_senders': senders\n",
        "    })\n",
        "final_profiles_basic = pd.DataFrame(profiles_list)\n",
        "\n",
        "# (b) Calculate average time to cash-out (the \"Time\" feature)\n",
        "print(\"   (b) Calculating average time to cash-out (avg_time_to_cash_out)...\")\n",
        "df_transfers = df[df['type'] == 'TRANSFER'][['step', 'nameDest']]\n",
        "df_cashouts = df[df['type'] == 'CASH_OUT'][['step', 'nameOrig']]\n",
        "df_transfers.rename(columns={'nameDest': 'user_id'}, inplace=True)\n",
        "df_cashouts.rename(columns={'nameOrig': 'user_id'}, inplace=True)\n",
        "df_transfers['tx_type'] = 'TRANSFER_IN'\n",
        "df_cashouts['tx_type'] = 'CASH_OUT'\n",
        "\n",
        "user_log = pd.concat([df_transfers, df_cashouts]).sort_values(by=['user_id', 'step'])\n",
        "user_log['prev_step'] = user_log.groupby('user_id')['step'].shift(1)\n",
        "user_log['prev_type'] = user_log.groupby('user_id')['tx_type'].shift(1)\n",
        "user_log['time_since_transfer'] = user_log['step'] - user_log['prev_step']\n",
        "is_pattern = (user_log['tx_type'] == 'CASH_OUT') & (user_log['prev_type'] == 'TRANSFER_IN')\n",
        "pattern_times = user_log[is_pattern]\n",
        "\n",
        "avg_time_profile = pattern_times.groupby('user_id')['time_since_transfer'].mean().reset_index()\n",
        "avg_time_profile.columns = ['user_id', 'avg_time_to_cash_out']\n",
        "\n",
        "# (c) Aggregate final profiles\n",
        "print(\"   (c) Aggregating final profiles...\")\n",
        "final_profiles = pd.merge(final_profiles_basic, avg_time_profile, on='user_id', how='left')\n",
        "final_profiles['avg_time_to_cash_out'] = final_profiles['avg_time_to_cash_out'].fillna(999)\n",
        "print(\"Full behavioral profiles created successfully.\")\n",
        "\n",
        "# --- Step 4: Create the \"Smart Sample\" for Training/Testing ---\n",
        "print(\"\\n--- Step 4: Creating the 'Smart Sample' ---\")\n",
        "# We do this step *after* building profiles to ensure our features are complete\n",
        "df_fraud = df[df['isFraud'] == 1]\n",
        "fraud_dest_ids = df_fraud['nameDest'].unique()\n",
        "fraud_orig_ids = df_fraud['nameOrig'].unique()\n",
        "all_fraud_user_ids = np.union1d(fraud_dest_ids, fraud_orig_ids)\n",
        "df_fraud_lifecycle = df[\n",
        "    df['nameOrig'].isin(all_fraud_user_ids) |\n",
        "    df['nameDest'].isin(all_fraud_user_ids)\n",
        "]\n",
        "df_normal = df[df['isFraud'] == 0]\n",
        "sample_size = min(500000, len(df_normal))\n",
        "df_normal_sample = df_normal.sample(n=sample_size, random_state=42)\n",
        "df_smart_sample = pd.concat([df_fraud_lifecycle, df_normal_sample]).drop_duplicates(keep='first')\n",
        "print(f\"The final 'Smart Sample' was created with {len(df_smart_sample)} rows.\")\n",
        "\n",
        "# --- Step 5: Merge Features and Prepare Final Dataset ---\n",
        "print(\"\\n--- Step 5: Merging Features and Preparing Final Dataset ---\")\n",
        "df_model_data = pd.merge(df_smart_sample, final_profiles, left_on='nameDest', right_on='user_id', how='left')\n",
        "df_model_data = pd.merge(df_model_data, final_profiles, left_on='nameOrig', right_on='user_id', how='left', suffixes=('_dest', '_orig'))\n",
        "\n",
        "# Fill NaNs created by merging\n",
        "df_model_data['dest_cash_out_ratio_dest'] = df_model_data['dest_cash_out_ratio_dest'].fillna(0)\n",
        "df_model_data['dest_unique_senders_dest'] = df_model_data['dest_unique_senders_dest'].fillna(0)\n",
        "df_model_data['avg_time_to_cash_out_dest'] = df_model_data['avg_time_to_cash_out_dest'].fillna(999)\n",
        "df_model_data['dest_cash_out_ratio_orig'] = df_model_data['dest_cash_out_ratio_orig'].fillna(0)\n",
        "df_model_data['dest_unique_senders_orig'] = df_model_data['dest_unique_senders_orig'].fillna(0)\n",
        "df_model_data['avg_time_to_cash_out_orig'] = df_model_data['avg_time_to_cash_out_orig'].fillna(999)\n",
        "df_model_data = df_model_data.dropna(subset=['isFraud']) # Drop any rows with missing labels\n",
        "\n",
        "# Define the features (X) and the label (y)\n",
        "features_list = [\n",
        "    'amount',\n",
        "    'dest_cash_out_ratio_dest',\n",
        "    'dest_unique_senders_dest',\n",
        "    'avg_time_to_cash_out_dest',\n",
        "    'dest_cash_out_ratio_orig',\n",
        "    'dest_unique_senders_orig',\n",
        "    'avg_time_to_cash_out_orig'\n",
        "]\n",
        "df_model_data['type_encoded'] = df_model_data['type'].astype('category').cat.codes\n",
        "features_list.append('type_encoded')\n",
        "\n",
        "X_all_features = df_model_data[features_list]\n",
        "y_all_labels = df_model_data['isFraud']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_all_features_scaled = scaler.fit_transform(X_all_features)\n",
        "\n",
        "# Split into Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all_features_scaled, y_all_labels,\n",
        "    test_size=0.3, random_state=42, stratify=y_all_labels\n",
        ")\n",
        "print(f\"Data prepared. Training on {len(X_train)} rows, testing on {len(X_test)} rows.\")\n",
        "\n",
        "# --- Step 6: Run All Experiments ---\n",
        "print(\"\\n--- Step 6: Running All Experiments ---\")\n",
        "\n",
        "# We will store all our F1 scores here\n",
        "final_results = {}\n",
        "\n",
        "# --- Experiment 1: Isolation Forest (Unsupervised) ---\n",
        "print(\"\\nRunning Experiment 1: Isolation Forest...\")\n",
        "contamination = y_train.mean() # Calculate contamination from the training set\n",
        "model_if = IsolationForest(contamination=contamination, random_state=42)\n",
        "model_if.fit(X_train) # Fit on training data\n",
        "predictions_if_raw = model_if.predict(X_test) # Predict on new data\n",
        "y_pred_if = [1 if p == -1 else 0 for p in predictions_if_raw]\n",
        "f1_if = f1_score(y_test, y_pred_if)\n",
        "final_results['IsolationForest'] = f1_if\n",
        "print(f\"Isolation Forest F1-Score: {f1_if:.2%}\")\n",
        "# --- Experiment 2: Local Outlier Factor (LOF) (Unsupervised) ---\n",
        "print(\"\\nRunning Experiment 2: Local Outlier Factor (LOF)...\")\n",
        "# Note: LOF is VERY slow. We will use a smaller sample for this one test.\n",
        "# We create a 10k sample just for this model\n",
        "X_test_sample, _, y_test_sample, _ = train_test_split(X_test, y_test, train_size=10000, random_state=42, stratify=y_test)\n",
        "model_lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n",
        "model_lof.fit(X_train) # Fit on training data\n",
        "predictions_lof_raw = model_lof.predict(X_test_sample) # Predict on the 10k sample\n",
        "y_pred_lof = [1 if p == -1 else 0 for p in predictions_lof_raw]\n",
        "f1_lof = f1_score(y_test_sample, y_pred_lof)\n",
        "final_results['LOF'] = f1_lof\n",
        "print(f\"LOF F1-Score (on 10k sample): {f1_lof:.2%}\")\n",
        "\n",
        "# --- Experiment 3: Autoencoder (Unsupervised) ---\n",
        "print(\"\\nRunning Experiment 3: Autoencoder...\")\n",
        "# We must train the Autoencoder *only* on normal data\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "input_dim = X_train.shape[1]\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "encoder = Dense(input_dim // 2, activation='relu')(input_layer)\n",
        "encoder = Dense(input_dim // 4, activation='relu')(encoder)\n",
        "decoder = Dense(input_dim // 2, activation='relu')(encoder)\n",
        "decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "autoencoder.fit(X_train_normal, X_train_normal, epochs=10, batch_size=32, validation_data=(X_test, X_test), verbose=0, shuffle=True)\n",
        "print(\"Autoencoder training complete.\")\n",
        "# Calculate reconstruction error\n",
        "train_predictions_ae = autoencoder.predict(X_train_normal)\n",
        "train_mse = np.mean(np.power(X_train_normal - train_predictions_ae, 2), axis=1)\n",
        "threshold = np.quantile(train_mse, 0.99) # Set threshold at 99th percentile of normal error\n",
        "predictions_ae = autoencoder.predict(X_test)\n",
        "test_mse = np.mean(np.power(X_test - predictions_ae, 2), axis=1)\n",
        "y_pred_ae = [1 if e > threshold else 0 for e in test_mse]\n",
        "f1_ae = f1_score(y_test, y_pred_ae)\n",
        "final_results['Autoencoder'] = f1_ae\n",
        "print(f\"Autoencoder F1-Score: {f1_ae:.2%}\")\n",
        "\n",
        "\n",
        "# --- Experiment 4: RandomForest (Supervised - The \"Proof\") ---\n",
        "print(\"\\nRunning Experiment 4: RandomForest (Supervised)...\")\n",
        "model_rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "model_rf.fit(X_train, y_train) # Fit on training data (with labels)\n",
        "predictions_rf = model_rf.predict(X_test) # Predict on new data\n",
        "f1_rf = f1_score(y_test, predictions_rf)\n",
        "final_results['RandomForest (Supervised)'] = f1_rf\n",
        "print(f\"RandomForest F1-Score: {f1_rf:.2%}\")\n",
        "\n",
        "# --- Experiment 5: XGBoost (Supervised - The \"Ultimate Proof\") ---\n",
        "print(\"\\nRunning Experiment 5: XGBoost (Supervised)...\")\n",
        "# Initialize XGBoost, using 'scale_pos_weight' for class imbalance (similar to class_weight='balanced' in RF)\n",
        "scale_pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "model_xgb = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=100,\n",
        "    # This parameter helps handle the imbalance, similar to class_weight in RF\n",
        "    scale_pos_weight=scale_pos_weight_value\n",
        ")\n",
        "\n",
        "# Fit on training data (with labels)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on new data\n",
        "predictions_xgb = model_xgb.predict(X_test)\n",
        "\n",
        "# Calculate F1-Score\n",
        "f1_xgb = f1_score(y_test, predictions_xgb)\n",
        "\n",
        "# Store and print result\n",
        "final_results['XGBoost (Supervised)'] = f1_xgb\n",
        "print(f\"XGBoost F1-Score: {f1_xgb:.2%}\")\n",
        "\n",
        "# --- Step 7: Final Conclusion ---\n",
        "# --- Step 7: Final Conclusion ---\n",
        "print(\"\\n--- Step 7: Final Comparative Results ---\")\n",
        "print(\"This is the final 'story' of the research paper.\")\n",
        "print(\"\\n=======================================================\")\n",
        "print(\"           FINAL MODEL F1-SCORES           \")\n",
        "print(\"=======================================================\")\n",
        "print(f\"1. Isolation Forest (Unsupervised): {final_results['IsolationForest']:.2%}\")\n",
        "print(f\"2. LOF (Unsupervised):                {final_results['LOF']:.2%}\")\n",
        "print(f\"3. Autoencoder (Unsupervised):        {final_results['Autoencoder']:.2%}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(f\"4. RandomForest (Supervised):         {final_results['RandomForest (Supervised)']:.2%}\")\n",
        "# Add XGBoost here:\n",
        "print(f\"5. XGBoost (Supervised):              {final_results['XGBoost (Supervised)']:.2%}\")\n",
        "print(\"=======================================================\")\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"The Unsupervised models (IF, LOF, AE) all 'failed' (low F1-Scores),\")\n",
        "print(\"proving they are 'confused' by the Behavioral Mimicry (mules vs. merchants).\")\n",
        "print(\"\\nThe Supervised model (RandomForest) 'succeeded' (high F1-Score),\")\n",
        "print(\"proving that our Behavioral Features ARE predictive and valuable.\")\n",
        "print(\"\\nThis proves that the problem is too complex for simple unsupervised models\")\n",
        "print(\"and that future work requires more advanced methods (like GNNs).\")\n",
        "\n",
        "print(\"\\n--- Full Research Pipeline Complete ---\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Final Research Pipeline ---\n",
            "Objective: Run all experiments step-by-step on the full, original dataset.\n",
            "\n",
            "--- Step 1: Load Data ---\n",
            "Successfully loaded the full file (6362620 rows).\n",
            "\n",
            "--- Step 2: The 'Critical Pivot' (Proof) ---\n",
            "Total Senders: 6353307\n",
            "Senders with only 1 transaction: 6344009\n",
            "Result: 99.85% of senders have no history.\n",
            "Conclusion: Methodology MUST pivot to profiling the recipient ('nameDest').\n",
            "\n",
            "--- Step 3: Building Behavioral Profiles (from all 6.3M rows) ---\n",
            "   (a) Calculating cash-out ratio and unique senders...\n",
            "   (b) Calculating average time to cash-out (avg_time_to_cash_out)...\n",
            "   (c) Aggregating final profiles...\n",
            "Full behavioral profiles created successfully.\n",
            "\n",
            "--- Step 4: Creating the 'Smart Sample' ---\n",
            "The final 'Smart Sample' was created with 561154 rows.\n",
            "\n",
            "--- Step 5: Merging Features and Preparing Final Dataset ---\n",
            "Data prepared. Training on 392807 rows, testing on 168347 rows.\n",
            "\n",
            "--- Step 6: Running All Experiments ---\n",
            "\n",
            "Running Experiment 1: Isolation Forest...\n",
            "Isolation Forest F1-Score: 18.39%\n",
            "\n",
            "Running Experiment 2: Local Outlier Factor (LOF)...\n",
            "LOF F1-Score (on 10k sample): 4.89%\n",
            "\n",
            "Running Experiment 3: Autoencoder...\n",
            "Autoencoder training complete.\n",
            "\u001b[1m12096/12096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step\n",
            "\u001b[1m5261/5261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 792us/step\n",
            "Autoencoder F1-Score: 19.57%\n",
            "\n",
            "Running Experiment 4: RandomForest (Supervised)...\n",
            "RandomForest F1-Score: 40.66%\n",
            "\n",
            "Running Experiment 5: XGBoost (Supervised)...\n",
            "XGBoost F1-Score: 16.91%\n",
            "\n",
            "--- Step 7: Final Comparative Results ---\n",
            "This is the final 'story' of the research paper.\n",
            "\n",
            "=======================================================\n",
            "           FINAL MODEL F1-SCORES           \n",
            "=======================================================\n",
            "1. Isolation Forest (Unsupervised): 18.39%\n",
            "2. LOF (Unsupervised):                4.89%\n",
            "3. Autoencoder (Unsupervised):        19.57%\n",
            "-------------------------------------------------------\n",
            "4. RandomForest (Supervised):         40.66%\n",
            "5. XGBoost (Supervised):              16.91%\n",
            "=======================================================\n",
            "\n",
            "Conclusion:\n",
            "The Unsupervised models (IF, LOF, AE) all 'failed' (low F1-Scores),\n",
            "proving they are 'confused' by the Behavioral Mimicry (mules vs. merchants).\n",
            "\n",
            "The Supervised model (RandomForest) 'succeeded' (high F1-Score),\n",
            "proving that our Behavioral Features ARE predictive and valuable.\n",
            "\n",
            "This proves that the problem is too complex for simple unsupervised models\n",
            "and that future work requires more advanced methods (like GNNs).\n",
            "\n",
            "--- Full Research Pipeline Complete ---\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-4gr0GGysYu",
        "outputId": "b60f695a-3b8b-4f39-dcc8-e97c5b917cc1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}